% Chapter Template

\chapter{Scale up, limit texas holdem poker experiment} % Main chapter title

\label{scale-up}

\lhead{Chapter 5. \emph{Scale up, limit texas holdem experiment}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

\section{Introduction}
In this chapter, we will study how well our algorithm is scaling up on more complex games. To do this we are going to adapt our algorithm to make it works with the limit texas holdem poker variant. This game has an info set size of \(10^{14}\) and 5 available actions.

\section{Equilibrium approximation}
One of the most complex tasks is to approximate the equilibrium. As discussed previously, a lot of algorithm aims to find a good approximation of the equilibrium and it is an active field of research (ref: \ref{LR:section:imperfect-game-solving}). There is a lot of complex algorithm with specific optimization. Because it is not the focus of this research, we decided to use a simple algorithm called CFR (Counterfactual Regret Minimization).

Approximating the equilibrium takes a relatively high amount of time, to increase the performances we implemented it in C++ and bound it to our python program.

\section{Experiment}
\subsection{Setup} \label{scale-up:experiment:setup}
We used the same setup as for the kuhn poker experiment (ref: \ref{kuhnexperiment:setup}). We tried to run it with the following hyper-parameters. They were choose to balance the time a game takes by reducing the overall number of games in one iteration.
\begin{itemize}
    \item population size 10
    \item individual kept each iteration 3
    \item mutation rate 0.2
    \item number of games 10
    \item iteration 200
\end{itemize}

Running one iteration takes 2h30. Due to a lack of resources, I was not able to properly run all the iterations. However, we can still think about how realistic it is to run this algorithm.

\subsection{Number of trainable parameters}
The number of trainable parameters is simply the number of weights and bias in the neural network. Using the same NN layers like the one for the kuhn experiment \ref{kuhnexperiment:setup}, we were able to extract these numbers, you can find it in table \ref{tab:trainable-parameters}.

\begin{table}[h]
\begin{center}
\begin{tabular}{ |c|c| } 
 \hline
 Game & Number of trainable parameters \\ 
 \hline
 Kuhn poker & 754 \\ 
 \hline
 Limit texas holdem poker & 6000 \\ 
 \hline
\end{tabular}
\end{center}
\caption{Number of trainable parameters}
\label{tab:trainable-parameters}
\end{table}

We can notice that there are only 8 times more trainable parameters (\(6000/754 \approx 8\)). We can assume that the training will need 8 times more iterations, so we can approximate the training time: \(2h30 * 8 * 100 \approx 83 days\). However there is a huge margin to reduce this training time, it will be discussed in the next section \ref{scale-up:optimisation}.

\section{Optimisation} \label{scale-up:optimisation}
The \(83days\) of training were calculated given the specs of my computer (i7-9750H CPU @ 2.60GHz, rtx2060 GPU and 16GB of RAM).

\subsection{Multi-threading} 
The first way to drastically improve the performance would be by multi-threading all the games. Indeed almost all the training time is spent making agents playing against each other. Given this, the training time could theoretically be lowered to at least less than 30s per iteration in an optimum setup where each game has a dedicated thread. Even in a non-optimum setup, each thread reduces the training time.

\subsection{Equilibrium}
Another way to improve the algorithm performance on this poker version would be to optimize the time to compute the equilibrium approximation. Indeed, in this version, at least 80\% of the game time is spent computing the equilibrium. There is a famous algorithm called MCCFR (Monte Carlo Counterfactual Regret Minimization), studies have shown that its performance is at least 10 times better than the classic CFR \citep{MCCFRlanctot2009monte}, \citep{MCCFRlanctot2009monte}. Just with this update, this could theoretically bring the training time to \(\approx17days\) taking my computer for reference.

\section{Conclusion}
Scaling up the algorithm is doable, the number of trainable parameters seems to reasonably increase allowing complex games to be trainable. The scalability of the algorithm heavily relies on how fast it is possible to approximate the equilibrium. The more difficult the game will be, the more time it would require to compute the equilibrium and so slow down the training. By multi-threading the algorithm, with enough hardware resources, it is possible to train this algorithm on complex imperfect games in a reasonable amount of time.