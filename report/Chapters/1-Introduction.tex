\chapter{Introduction} % Main chapter title

\lhead{Chapter 1. \emph{Introduction}} % This is for the header on each page - perhaps a shortened title

Imperfect game solving is the term used to solve a game where some information is unknown such as cards in poker. Imperfect game solving is an interesting field because it can be transposed easily to real-world problems. For example, in military, sometimes the opponent's resources are unknown or with a vague estimate, imperfect game solving algorithms are designed to handle state with unknown information.

Research in this field has been mostly focused on finding the equilibrium of a game. An equilibrium strategy played on the long term can not lose. It is the probabilistic solution to the game. On larger game state equilibrium can not be computed, algorithms are so trying to approximate it and get closer to it.

However, an equilibrium strategy does not guarantee the higher reward against an opponent. It just guarantees that on the long term, it will not lose. To improve the reward against an opponent, we need to exploit it. This is done by modeling the opponent's behavior and exploiting the mistakes made by him, this is called a sub-optimal opponent.

The problem is that whenever we try to exploit an opponent we are exposed to exploitability, this is referred to as the get-taught-and-exploited problem \citep{SANDHOLM2007382}. On another side, some research has proven that in some imperfect games, safe exploitation is possible introducing the notion of gift \citep{ganzfried2015safe}. Safe exploitation ensures that the opponent can be exploited without being exposed to exploitability. In this research we will not focused on having a safe exploitation, this is a choice made to fully explore the capacity of neural network and not limit it by the safe exploitation constraints.

Existing research has been done on opponent modeling in imperfect games. As said previously one demonstrates that safe opponent exploitation exists in some imperfect games such as poker \citep{ganzfried2015safe}, however, the method used in this paper do not scale for large game state or complex games. One research has been done for modeling large game state (poker limit texas hold'em) \citep{ganzfried2011game}. Both methods use a similar method. It uses a decision tree with a pre-computed equilibrium approximation, it plays a certain number of games against an opponent and updates the statistical difference between the equilibrium and the opponent choices. This difference defines the opponent model. This algorithm have multiple limitations. First, it requires playing a relatively high number of times against an opponent to start having a good result. Secondly in a large game tree, only the first branches will have an accurate opponent modeling, further we go in the tree less the algorithm has data. This is a major problem because it only models the opponent early game. Third, this method does not encode complex behavior, it is limited to the statistical mistake of the opponent. Fourth, it does not have a spatial representation, it means that the first or last game played by the opponent has the same weight in the opponent modeling. For example, it will not be able to model anger, opponent betting higher than usual after losing.

The goal of this project is to demonstrate that opponent modeling is possible with neural network and to assess the performances. We are aiming to have an algorithm that is not game-dependent and which is working with information that is available or computable for every imperfect game. This information is for example equilibrium and the game result. This will allow this algorithm to be re-used for other use cases.

We will at first works on a small game state to makes our experimentation. It would allow us to iterate faster and have rapid feedback on the architecture performance. We will use Kuhn poker which is an extremely simple version of poker \citep{kuhn1950simplified}. Then we will try to scale up the algorithm to a larger game state to assess the scalability. We will use limit texas holdem poker which has an info set size of \( 10^{14}\).

We will use recurrent neural network (RNN) because it will be able to use its output as input and so update the opponent model given the previous model. Recurrent neural network has proven to be able to learn complex problems. We are going to use another neural network to take actions within the game given the opponent model.

We will evaluate our model playing Kuhn poker. It will play against simple opponents that are easily exploitable and more advanced opponent that has already been exploited with other technique to have a comparison point. To evaluate the exploitability of our algorithm we will make it play against an equilibrium opponent. In addition, we will use the true best response algorithm that is known for exploitability testing \citep{johanson2011accelerating}.

This report is structured as follows:
\begin{itemize}
    \item \textbf{Chapter \ref{LR}, Literature Review}: This chapter covers the literature regarding machine learning models that had been considered for this project. It also explains the research made in imperfect game solving.
    \item \textbf{Chapter \ref{Methodology}, Methodology}: This chapter explains the architecture of the two neural networks, the methods that will be used to train them, and the evaluation process.
    \item \textbf{Chapter \ref{kuhn-poker}, Kuhn poker experiment}: This chapter details how the algorithm performed playing Kuhn poker and assess the potential of NN to model opponent.
    \item \textbf{Chapter \ref{scale-up}, Scale-up, limit texas holdem poker experiment}: This chapter explores the scalability of the algorithm by making it play limit texas holdem poker.
\end{itemize}
