from src.environement.TexasHoldemLimit.strategy.all_strategy import ALL_TEXAS_HOLDEM_STRATEGY
from src.environement.Strategy import Strategy
from src.environement.Environment import Environment
from src.environement.strategys.human_strategy import HumanStrategy
from src.environement.strategys.close_equilibrium import CloseEquilibriumStrategy
from src.environement.TexasHoldemLimit.TexaxHoldemEnv import TexasHoldemEnv
from src.utils.paths import get_save_file
from src.nn.NN import DumbNNParams, HiddenLayersParams, NNParams
from src.report.Report import Report
from src.nn.NNStrategy import NNStrategy
from src.train.ea.Population import Population, TrainingIndividual
from src.train.ea.Individual import Individual
from src.train.ea.play_against import play_against
from src.environement.Kuhn.strategy.all_strategy import ALL_KUHN_STRATEGY
from src.environement.Kuhn.KuhnEnv import KuhnEnv
from typing import List, Tuple
import argparse
import json
import pokercfr

def parse_nn_parms(args, nn_suffix: str) -> NNParams:
    layers: List[HiddenLayersParams] =  []
    sizes: List[int] = getattr(args, f'layers_size_{nn_suffix}')
    activations: List[str] = getattr(args, f'layers_activation_{nn_suffix}')
    nb_layers: int = getattr(args, f'nb_hidden_layers_{nn_suffix}')
    if nb_layers != len(sizes):
        raise Exception('Invalid "layers-size" size')
    if nb_layers != len(activations):
        raise Exception('Invalid "layers-activation" size')
    for i in range(nb_layers):
        layers.append(HiddenLayersParams(sizes[i], activations[i]))
    params = NNParams(args.model_size, layers)
    return params

def get_env(args) -> Tuple[Environment, List[Strategy]]:
    if args.env == "kuhn":
        return KuhnEnv(), ALL_KUHN_STRATEGY
    elif args.env == "texas":
        return TexasHoldemEnv(), ALL_TEXAS_HOLDEM_STRATEGY
    else:
        raise Exception(f'Unkown env "{args.env}"')

def train(args):
    if args.new_individual_per_iteration >= args.training_nb:
        raise Exception("New individual per iteration can not be higher than the training number")
    env, strategys = get_env(args)

    teaching_individuals: List[Individual] = []
    for S in strategys:
        teaching_individuals.append(Individual(S()))
    for _ in range(args.suboptimal_agents_nb):
        teaching_individuals.append(Individual(CloseEquilibriumStrategy()))

    training_individuals: List[TrainingIndividual] = []
    for _ in range(args.training_nb):
        training_individuals.append(TrainingIndividual(args.model_size,
            NNStrategy(env, params_dm=parse_nn_parms(args, 'dm') ,params_om=parse_nn_parms(args, 'om'))))

    population = Population(
        training_individuals=training_individuals,
        new_individual_per_iteration=args.new_individual_per_iteration,
        mutation_rate=args.mutation_rate,
        teaching_set=teaching_individuals,
        nb_games=args.nb_games
    )
    for iteration_nb in range(args.iteration_nb):
        print(f'ITERATION {iteration_nb + 1}')
        population.process_iteration(env)
    individual = population.get_best_individual()
    s: NNStrategy = individual.startegy
    save_id = args.save_id if "save_id" in args else str(id(s))
    s.save(save_id)
    with open(get_save_file(save_id, "training-info.json"), "w+") as file:
        file.write(json.dumps(args.__dict__, indent=4))
    if args.report:
        analyze(save_id, env)

def analyze(
    save_id: str,
    env: Environment,
    nn_summary=True,
    best_reponse=True,
    reward_over_time=True,
):
    report = Report(
        save_id=save_id,
        env=env,
    )
    report.load_nn_strategy()
    if nn_summary:
        report.nn_summary(25)
    if best_reponse:
        report.best_response_report()
    if reward_over_time:
        report.plot_reward_over_time()

def play_with_human(save_id: str, nb_games=5):
    env = KuhnEnv()
    nn_strategy = NNStrategy(env, DumbNNParams(), DumbNNParams())
    nn_strategy.load(save_id)
    human_strategy = HumanStrategy()
    play_against(env, nn_strategy, human_strategy, nb_games, debug=True)

def parse_args():
    parser = argparse.ArgumentParser(prog="opponent-modelling-nn", formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument("--env", required=True, choices=["kuhn", "texas"], help="the poker environment to use")
    subparsers = parser.add_subparsers()

    train_parser = subparsers.add_parser("train", formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    train_parser.add_argument("--report", action="store_true", default=False, help="run report directly after")
    train_parser.add_argument("-id", "--save-id", help="the save id")

    nn_group = train_parser.add_argument_group("Neural Network")
    nn_group.add_argument("-m", "--model-size", type=int, help="size of the opponent model", default=5)
    for long, short in [("opponent modelling", "om"), ("descision making", "dm")]:
        nn_group.add_argument(
            f"--nb-hidden-layers-{short}",
            help=f"{long} number of hidden layers",
            type=int,
            default=1
        )
        nn_group.add_argument(
            f"--layers-activation-{short}",
            help=f"{long} activation function of the hidden layer(s)",
            choices=["relu", "linear"],
            nargs="+",
            default=["linear"]
        )
        nn_group.add_argument(
            f"--layers-size-{short}",
            help=f"{long} size of the hidden layer(s)",
            type=int,
            nargs="+",
            default=[10]
        )

    ea_group = train_parser.add_argument_group("Evolutionary Algorithm")
    ea_group.add_argument("-nb", "--training-nb", type=int, help="number of training individual", default=20)
    ea_group.add_argument("-new", "--new_individual-per-iteration", type=int, help="new individual per iteration",  default=15)
    ea_group.add_argument("-i", "--iteration-nb", help="number of iterations", type=int, default=100)
    ea_group.add_argument("-r", "--mutation-rate", type=float, help="mutation rate", default=0.1)
    ea_group.add_argument("--nb-games", help="number of games between agents each iteration ", type=int, default=25)
    ea_group.add_argument("--suboptimal-agents-nb", type=int, help="number of suboptimal agents", default=3)

    report_parser = subparsers.add_parser("report", formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    report_parser.add_argument("id", type=str, help="the save id")
    report_parser.add_argument("-nn", "--nn-summary", action="store_true", default=False)
    report_parser.add_argument("-b", "--best-response", action="store_true", default=False)
    report_parser.add_argument("-r", "--reward-over-time", action="store_true", default=False)
    report_parser.add_argument("-all", "--report-all", help="run all the reporting", action="store_true", default=False)

    return parser.parse_args()

def runCfr(
    publicCards,
    privateCards,
    betNb,
    chips1,
    a1hasplayed,
    a2hasplayed,
    chips2,
    ourTurn,
    iteration
):
    result: str = pokercfr.cfr(
        publicCards,
        privateCards,
        betNb,
        chips1,
        a1hasplayed,
        a2hasplayed,
        chips2,
        ourTurn,
        iteration
    )
    result = result[:-1]
    equilibrium = [float(x) for x in result.split(' ')]
    return equilibrium

def main():
    args = parse_args()
    print(args)
    if "training_nb" in args:
        train(args)
    else:
        env, _ = get_env(args)
        if args.report_all:
            analyze(args.id, env)
        else:
            analyze(
                args.id,
                env,
                nn_summary=args.nn_summary,
                best_reponse=args.best_response,
                reward_over_time=args.reward_over_time
            )

main()