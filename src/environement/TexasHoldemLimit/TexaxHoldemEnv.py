from __future__ import annotations
from ..Environment import Environment
from .TexasHoldemState import TexasHoldemState
from .TexasHoldemInfo import TexasHoldemAction, INITIAL_BET_TEXAS_LIMIT_BET
from .TexasHoldemDealer import TexasHoldemDealer
from .TexasHoldemAgent import TexasHodlemAgent
from .rank_hands import compare_hands_5, get_best_hand_7, HandCompareResult
from typing import List
import random
import numpy as np

class TexasHoldemEnv(Environment):
    actions_this_round: List[TexasHoldemAction]

    def generate_random_starting_state(self):
        return random.randint(0, 100000000)

    def set_starting_state(self, state, inverse_player):
        self._dealer.reset(state)
        a1 = self.agent1 if inverse_player == False else self.agent2
        a2 = self.agent2 if inverse_player == False else self.agent1
        a1.cards = [self._dealer.draw_card(), self._dealer.draw_card()]
        a2.cards = [self._dealer.draw_card(), self._dealer.draw_card()]

    def _create_dealer(self) -> TexasHoldemDealer:
        return TexasHoldemDealer()

    def create_state(self):
        self.state = TexasHoldemState(actions_number=len(list(TexasHoldemAction)), max_actions_in_game=8 * 4, game_outcome_size=1)

    def create_agents(self):
        self.agent1 = TexasHodlemAgent(self.state)
        self.agent2 = TexasHodlemAgent(self.state)

    def _round_is_finished(self) -> bool:
        return len(self.actions_this_round) >= 2 and self.agent1.chips_bet == self.agent2.chips_bet

    def _is_game_ended(self) -> bool:
        return (
            self.agent1.has_fold or self.agent2.has_fold or
            (len(self.state.public_cards) == 5 and self._round_is_finished())
        )

    def transform_action_to_enum(self, action):
        return TexasHoldemAction(action)

    def play(self, action):
        super().play(action)
        self.actions_this_round.append(action)
        if (self._round_is_finished() and len(self.state.public_cards) < 5):
            self.state.public_cards.append(self._dealer.draw_card())
            self.actions_this_round = []

    def get_available_actions(self) -> List[TexasHoldemAction]:
        a = self.get_playing_agent()
        o = self.get_opponent(a)
        actions: List[TexasHoldemAction] = []
        if (a.chips_bet < o.chips_bet):
            actions = [TexasHoldemAction.CALL, TexasHoldemAction.FOLD, TexasHoldemAction.RAISE]
        else:
            actions = [TexasHoldemAction.BET, TexasHoldemAction.CHECK]
        if (self.get_playing_agent().bet_count_this_round >= 4):
            actions = filter(lambda x: x not in [TexasHoldemAction.BET, TexasHoldemAction.RAISE], actions)
        return actions

    def next_game(self, p1_start=True):
        super().next_game(p1_start)
        self.get_playing_agent().chips_bet = INITIAL_BET_TEXAS_LIMIT_BET
        self.get_opponent(self.get_playing_agent()).chips_bet = INITIAL_BET_TEXAS_LIMIT_BET / 2
        self.actions_this_round = []
        self.agent1.cards = [self._dealer.draw_card(), self._dealer.draw_card()]
        self.agent2.cards = [self._dealer.draw_card(), self._dealer.draw_card()]

    def _on_game_end(self):
        winner = None
        if self.agent1.has_fold:
            winner = self.agent1
        elif self.agent2.has_fold:
            winner = self.agent2
        else:
            self.agent1.revealed = True
            self.agent2.revealed = True
            result = compare_hands_5(
                get_best_hand_7(self.agent1.cards + self.state.public_cards),
                get_best_hand_7(self.agent2.cards + self.state.public_cards)
            )
            if (result == HandCompareResult.WIN):
                winner = self.agent1
            elif (result == HandCompareResult.LOSS):
                winner = self.agent2
        if (winner):
            looser = self.get_opponent(winner)
            winner.reward = looser.chips_bet
            looser.reward = -looser.chips_bet

    def create_new(self) -> TexasHoldemEnv:
        return TexasHoldemEnv()

    def game_outcome_array_for_agent(self, agent: TexasHodlemAgent) -> np.ndarray:
        return np.array([agent.reward])