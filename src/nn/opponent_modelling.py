from src.environement.Agent import Agent
from src.environement.Environment import Environment
import numpy as np
from tensorflow import keras
from tensorflow.keras import layers
from .NN import NN
from typing import List


class OpponentModellingNN(NN):
    def get_input_size(self):
        game_history_size = self.game_info.available_action_size * \
            self.game_info.player_max_action_per_game * 4
        # *4 -> own_action + opponent_action + opponent equilibrium private + opponent equilibrium public
        return game_history_size + self.game_info.game_outcome_size + self.params.modelling_size

    def _create_model(self):
        input_size = self.get_input_size()
        self.model = keras.Sequential(
            [layers.Input(shape=(input_size,))] +
            self._get_hidden_layers() +
            [layers.Dense(self.params.modelling_size, activation='sigmoid')]
        )

    def get_input_labels(self) -> List[str]:
        total_actions_size = self.game_info.available_action_size * \
            self.game_info.player_max_action_per_game
        return (
            [f"agent-action-{i}" for i in range(total_actions_size)] +
            [f"opponent-action-{i}"for i in range(total_actions_size)] +
            [f"opponent-private-equilibrium-{i}"for i in range(total_actions_size)] +
            [f"opponent-public-equilibrium-{i}"for i in range(total_actions_size)] +
            [f"game-outcome-{i}"for i in range(self.game_info.game_outcome_size)] +
            [f"opponent-model-{i}"for i in range(self.params.modelling_size)]
        )

    def get_output_labels(self) -> List[str]:
        return [f'opponent-model-{i}' for i in range(self.params.modelling_size)]

def create_opponent_modelling_input(
        env: Environment, agent: Agent, opponent_model: np.ndarray):
    opponent = env.get_opponent(agent)
    return np.concatenate((
        agent.actions_np.flatten(),
        opponent.actions_np.flatten(),
        opponent.private_equilibrium.flatten() if opponent.revealed else np.zeros(
            len(opponent.private_equilibrium.flatten())),
        opponent.public_equilibrium.flatten(),
        env.game_outcome_array_for_agent(agent),
        opponent_model,
    ))
