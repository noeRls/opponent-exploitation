from typing import List
from src.environement.Agent import Agent
from src.environement.Environment import Environment
import numpy as np
from tensorflow import keras
from tensorflow.keras import layers
from .NN import NN


class DescisionMakingNN(NN):
    def get_input_size(self):
        game_history_size = self.game_info.available_action_size * \
            self.game_info.player_max_action_per_game * 2
        # *3 -> own action + opponent action + public opponent equilibrium
        return (
            game_history_size +
            # current equilibrium + available action
            self.game_info.available_action_size * 2 +
            self.modelling_size
        )

    def _create_model(self):
        input_size = self.get_input_size()
        self.model = keras.Sequential([
            layers.Input(shape=(input_size,)),
            layers.Dense(10),
            layers.Dense(self.game_info.available_action_size,
                         activation='softmax'),
        ])

    def get_input_labels(self) -> List[str]:
        total_actions_size = self.game_info.available_action_size * \
            self.game_info.player_max_action_per_game
        return (
            [f"available-action-{i}" for i in range(self.game_info.available_action_size)] +
            # [f"agent-action-{i}" for i in range(total_actions_size)] +
            [f"opponent-action-{i}"for i in range(total_actions_size)] +
            [f"opponent-public-equilibrium-{i}"for i in range(total_actions_size)] +
            [f"own-equilibrium-{i}"for i in range(self.game_info.available_action_size)] +
            [f"opponent-model-{i}"for i in range(self.modelling_size)]
        )

    def get_output_labels(self) -> List[str]:
        return ['action-check', 'action-bet', 'action-fold']

def create_descision_making_nn_input(
        env: Environment, agent: Agent, opponent_model: np.ndarray):
    available_actions = env.get_available_actions()
    available_actions_array = np.zeros(env.state.actions_number)
    for action in available_actions:
        available_actions_array[action.value] = 1
    opponent = env.get_opponent(agent)
    return np.concatenate((
        available_actions_array,
        # agent.actions_np.flatten(),
        opponent.actions_np.flatten(),
        opponent.public_equilibrium.flatten(),
        agent.compute_private_equilibrium().flatten(),
        opponent_model
    ))
