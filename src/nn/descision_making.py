from src.environement.Agent import Agent
from src.environement.Environment import Environment
import numpy as np
from tensorflow import keras
from tensorflow.keras import layers
from .NN import NN

class DescisionMakingNN(NN):
    def get_input_size(self):
        game_history_size = self.game_info.available_action_size * self.game_info.player_max_action_per_game * 3
        # *3 -> own action + opponent action + public opponent equilibrium
        return (
            game_history_size +
            self.game_info.available_action_size + # current equilibrium
            self.modelling_size
        )

    def _create_model(self):
        input_size = self.get_input_size()
        self.model = keras.Sequential(
            [layers.Dense(input_size)],
            [layers.Dense(10)],
            [layers.Softmax(self.game_info.available_action_size)],
        )

def create_descision_making_nn_input(env: Environment, agent: Agent, opponent_model: np.ndarray):
    available_actions = env.get_available_actions()
    available_actions_array = np.zeros(env._state.actions_number)
    for action in available_actions:
        available_actions_array[action.value] = 1
    opponent = env.get_opponent(agent)
    return np.concatenate((
        available_actions_array,
        agent.actions_np,
        opponent.actions_np,
        opponent.public_equilibrium,
        agent._compute_private_equilibrium(),
        opponent_model
    ))