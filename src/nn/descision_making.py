from src.environement.Agent import Agent
from src.environement.Environment import Environment
import numpy as np
from tensorflow import keras
from tensorflow.keras import layers
from .NN import NN

class DescisionMakingNN(NN):
    def get_input_size(self):
        game_history_size = self.game_info.available_action_size * self.game_info.player_max_action_per_game * 3
        # *3 -> own action + opponent action + public opponent equilibrium
        return (
            game_history_size +
            self.game_info.available_action_size * 2 + # current equilibrium + available action
            self.modelling_size
        )

    def _create_model(self):
        input_size = self.get_input_size()
        self.model = keras.Sequential([
            layers.Input(shape=(input_size,)),
            layers.Dense(10),
            layers.Dense(self.game_info.available_action_size, activation='softmax'),
        ])

def create_descision_making_nn_input(env: Environment, agent: Agent, opponent_model: np.ndarray):
    available_actions = env.get_available_actions()
    available_actions_array = np.zeros(env._state.actions_number)
    for action in available_actions:
        available_actions_array[action.value] = 1
    opponent = env.get_opponent(agent)
#     print(f'''
# available_actions_array={available_actions_array}
# agent.actions_np.flatten()={agent.actions_np.flatten()}
# opponent.actions_np.flatten()={opponent.actions_np.flatten()}
# opponent.public_equilibrium.flatten()={opponent.public_equilibrium.flatten()}
# agent._compute_private_equilibrium().flatten()={agent._compute_private_equilibrium().flatten()}
# opponent_mode={opponent_model}
#     ''')
    return np.concatenate((
        available_actions_array,
        agent.actions_np.flatten(),
        opponent.actions_np.flatten(),
        opponent.public_equilibrium.flatten(),
        agent._compute_private_equilibrium().flatten(),
        opponent_model
    ))